"""
Film generation endpoints for AI video workflow.
Phase 4: Generate and assemble video shots using per-scene reference generation.
"""
import os
import uuid
import asyncio
import httpx
from datetime import datetime
from typing import Dict, Optional, List, Literal
from dataclasses import dataclass, field
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel

import json

from ..core import (
    generate_video,
    generate_text,
    generate_image_with_references,
    assemble_videos,
    COST_IMAGE_GENERATION,
    COST_VIDEO_VEO_FAST_PER_SECOND,
)

# Video duration and cost
VIDEO_DURATION_SECONDS = 8
COST_PER_VIDEO = VIDEO_DURATION_SECONDS * COST_VIDEO_VEO_FAST_PER_SECOND  # $1.20 per 8s clip

# Testing mode: limit shots for faster iteration
MAX_SHOTS_FOR_TESTING = 3  # Set to None for full film generation
SCENE_REFS_PER_SHOT = 3    # Nano Banana generates 3 scene-specific refs per shot

# Veo rate limiting: 2 RPM on Paid Tier 1
# Semaphore limits concurrent Veo calls; delay spaces them within the minute
VEO_MAX_CONCURRENT = 2
VEO_DELAY_BETWEEN_CALLS = 32  # seconds — ensures 2 calls fit within 60s window
_veo_semaphore: Optional[asyncio.Semaphore] = None

def _get_veo_semaphore() -> asyncio.Semaphore:
    """Lazy-init semaphore (must be created inside a running event loop)."""
    global _veo_semaphore
    if _veo_semaphore is None:
        _veo_semaphore = asyncio.Semaphore(VEO_MAX_CONCURRENT)
    return _veo_semaphore
from ..config import TEMP_DIR, GOOGLE_GENAI_API_KEY
from .story import Story, Beat, SceneBlock
from .moodboard import ApprovedVisuals, ReferenceImage

router = APIRouter()


# ============================================================
# Constants
# ============================================================

STYLE_PREFIXES = {
    "cinematic": "Cinematic, photorealistic, 35mm film, shallow depth of field, natural lighting.",
    "3d_animated": "3D animated, Pixar-style, stylized realism, expressive, vibrant colors.",
    "2d_animated": "2D animated, hand-drawn, bold outlines, stylized, flat lighting.",
}


# Beat-type to cinematography defaults (from prompting guide)
BEAT_CINEMATOGRAPHY = {
    "hook":  {"shot": "Medium close-up", "angle": "Eye level",  "movement": "Push in",          "energy": "High"},
    "rise":  {"shot": "Medium close-up", "angle": "Eye level",  "movement": "Slow push in",     "energy": "Building"},
    "spike": {"shot": "Close-up",        "angle": "Eye level",  "movement": "Push in tight",    "energy": "Peak"},
    "drop":  {"shot": "Medium close-up", "angle": "Eye level",  "movement": "Slow pull back",   "energy": "Sinking"},
    "cliff": {"shot": "Wide shot",       "angle": "Eye level",  "movement": "Static",           "energy": "Shock"},
}

class DirectorScript(BaseModel):
    """Per-scene director instructions generated by AI. Invisible to user."""
    scene_number: int
    shot_type: str          # "Medium close-up", "Close-up", "Wide shot"
    camera_angle: str       # "Eye level", "Low angle", "High angle"
    camera_movement: str    # "Slow push in", "Static", "Pull back"
    sound_design: str       # "Photo slapping granite. Tense silence."
    dialogue_delivery: Optional[List[dict]] = None  # [{"character": "Elena", "verb": "says coldly"}]
    style_note: str = ""    # "Tense, claustrophobic"


# ============================================================
# Request/Response Models
# ============================================================

class KeyMomentRef(BaseModel):
    """Key moment image for video reference (SPIKE - emotional peak)."""
    image_base64: str
    mime_type: str


class GenerateFilmRequest(BaseModel):
    story: Story
    approved_visuals: ApprovedVisuals
    key_moment_image: KeyMomentRef  # Single SPIKE key moment for video reference
    beat_numbers: Optional[List[int]] = None  # If provided, only preview these beats
    generation_id: Optional[str] = None  # Link to generation session


class GenerateFilmResponse(BaseModel):
    film_id: str
    status: str
    total_shots: int


class RegenerateShotRequest(BaseModel):
    """Request to regenerate a specific shot."""
    feedback: Optional[str] = None  # Optional feedback to modify the shot


class RegenerateShotResponse(BaseModel):
    """Response from shot regeneration."""
    film_id: str
    shot_number: int
    status: str
    preview_url: str


class CompletedShotInfo(BaseModel):
    number: int
    preview_url: str
    veo_prompt: Optional[str] = None


class CostBreakdown(BaseModel):
    scene_refs_usd: float
    videos_usd: float
    total_usd: float


class FilmStatusResponse(BaseModel):
    film_id: str
    status: Literal["generating", "assembling", "ready", "failed", "interrupted"]
    current_shot: int
    total_shots: int
    phase: Literal["keyframe", "filming", "assembling"]
    completed_shots: List[CompletedShotInfo]
    final_video_url: Optional[str]
    error_message: Optional[str]
    cost: CostBreakdown


# ============================================================
# In-Memory Job Storage
# ============================================================

@dataclass
class CompletedShot:
    number: int
    video_path: str
    veo_prompt: str = ""


@dataclass
class FilmJob:
    film_id: str
    status: Literal["generating", "assembling", "ready", "failed"]
    created_at: datetime

    # Input data
    story: Story
    approved_visuals: ApprovedVisuals
    key_moment_image: KeyMomentRef  # Single SPIKE key moment for video reference

    # Progress tracking
    total_shots: int
    current_shot: int = 0
    phase: Literal["keyframe", "filming", "assembling"] = "keyframe"

    # Completed work
    completed_shots: List[CompletedShot] = field(default_factory=list)

    # Output
    final_video_path: Optional[str] = None

    # Error tracking
    error_message: Optional[str] = None

    # Cost tracking (USD)
    cost_scene_refs: float = 0.0
    cost_videos: float = 0.0

    # Link to generation session (for persistence)
    generation_id: Optional[str] = None

    @property
    def cost_total(self) -> float:
        return self.cost_scene_refs + self.cost_videos


# In-memory storage (write-through cache — backed by SQLite)
film_jobs: Dict[str, FilmJob] = {}


async def persist_film_job(job: FilmJob) -> None:
    """No-op: persistence moved to Supabase on the frontend side."""
    pass


# ============================================================
# Helper Functions
# ============================================================

def default_director_script(beat: Beat) -> DirectorScript:
    """Fallback director script from beat_type cinematography defaults."""
    bt = beat.beat_type or "rise"
    cine = BEAT_CINEMATOGRAPHY.get(bt, BEAT_CINEMATOGRAPHY["rise"])
    return DirectorScript(
        scene_number=beat.number,
        shot_type=cine["shot"],
        camera_angle=cine["angle"],
        camera_movement=cine["movement"],
        sound_design="Ambient sounds matching the scene.",
        style_note=cine["energy"],
    )


async def generate_director_scripts(story: Story, beats: List[Beat]) -> List[DirectorScript]:
    """Generate director scripts for all beats in a single Gemini Flash call.

    Uses the beat-to-cinematography mapping as guidance but allows AI to
    deviate based on scene content. Falls back to defaults on failure.
    """
    # Build scene summaries for the prompt
    scene_summaries = []
    for beat in beats:
        bt = beat.beat_type or "rise"
        cine = BEAT_CINEMATOGRAPHY.get(bt, BEAT_CINEMATOGRAPHY["rise"])

        # Get scene content from blocks
        content_lines = []
        dialogue_chars = []
        if beat.blocks:
            for block in beat.blocks:
                if block.type == "description":
                    content_lines.append(block.text)
                elif block.type == "action":
                    content_lines.append(f"[Action] {block.text}")
                elif block.type == "dialogue" and block.character:
                    content_lines.append(f'{block.character}: "{block.text}"')
                    dialogue_chars.append(block.character)
        else:
            if beat.description:
                content_lines.append(beat.description)
            if beat.action:
                content_lines.append(f"[Action] {beat.action}")
            if beat.dialogue:
                for d in beat.dialogue:
                    content_lines.append(f'{d.character}: "{d.line}"')
                    dialogue_chars.append(d.character)

        scene_summaries.append(
            f"Scene {beat.number} (energy: {cine['energy']}, default camera: {cine['shot']}/{cine['movement']}):\n"
            f"  Heading: {beat.scene_heading or 'N/A'}\n"
            f"  Content: {chr(10).join(content_lines)}\n"
            f"  Characters speaking: {', '.join(dialogue_chars) if dialogue_chars else 'None'}"
        )

    prompt = f"""You are a cinematographer creating shot-by-shot director notes for an 8-scene short film.

SCENES:
{chr(10).join(scene_summaries)}

CHARACTERS IN FILM:
{chr(10).join(f'- {c.name} ({c.age} {c.gender}): {c.appearance}' for c in story.characters)}

For each scene, provide director instructions as a JSON array. Each element:
{{
  "scene_number": <int>,
  "shot_type": "<e.g. Medium close-up, Close-up, Wide shot, Over-the-shoulder>",
  "camera_angle": "<e.g. Eye level, Low angle, High angle, Dutch angle>",
  "camera_movement": "<ONE specific movement: Slow push in, Static, Pull back, Pan left, Track right>",
  "sound_design": "<specific sounds for this scene, e.g. 'Rain pattering on glass. Muffled city traffic.'>",
  "dialogue_delivery": [<for each speaking character: {{"character": "Name", "verb": "says coldly"}}>] or null,
  "style_note": "<one-phrase mood/atmosphere, e.g. 'Tense, claustrophobic'>"
}}

RULES:
- ONE camera movement per scene (golden rule: one camera verb, one lighting motif, one action)
- Sound design should be specific and concrete, not generic
- Dialogue delivery verbs should match emotional tone (whispers, snaps, mutters, pleads, says flatly)
- Use the default camera suggestions as starting points but deviate when the scene demands it
- Keep style_note to 2-4 words max

Return ONLY the JSON array, no markdown fences or extra text."""

    try:
        response = await generate_text(prompt=prompt, model="gemini-2.0-flash")

        # Clean response - strip markdown fences if present
        cleaned = response.strip()
        if cleaned.startswith("```"):
            cleaned = cleaned.split("\n", 1)[1] if "\n" in cleaned else cleaned[3:]
        if cleaned.startswith("json"):
            cleaned = cleaned[4:].strip()
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3].strip()

        scripts_data = json.loads(cleaned)

        # Build DirectorScript objects
        scripts = []
        for sd in scripts_data:
            scripts.append(DirectorScript(
                scene_number=sd.get("scene_number", 0),
                shot_type=sd.get("shot_type", "Medium close-up"),
                camera_angle=sd.get("camera_angle", "Eye level"),
                camera_movement=sd.get("camera_movement", "Static"),
                sound_design=sd.get("sound_design", "Ambient sounds."),
                dialogue_delivery=sd.get("dialogue_delivery"),
                style_note=sd.get("style_note", ""),
            ))

        print(f"Director scripts generated: {len(scripts)} scenes")
        return scripts

    except Exception as e:
        print(f"WARNING: Director script generation failed ({e}), using defaults")
        return [default_director_script(beat) for beat in beats]


async def generate_scene_references(
    beat: Beat,
    story: Story,
    approved_visuals: ApprovedVisuals,
    director_script: Optional[DirectorScript] = None,
) -> List[dict]:
    """Generate 3 scene-specific reference images via Nano Banana (Gemini image gen).

    For each shot, selects the relevant character refs + location ref from the
    approved moodboard, then generates 3 composite images showing those characters
    in that location from different angles. These 3 scene refs are what Veo receives.

    Returns:
        List of 3 dicts, each with image_base64 and mime_type
    """
    style_prefix = STYLE_PREFIXES.get(story.style, STYLE_PREFIXES["cinematic"])

    # 1. Select character refs for this scene
    char_refs = []
    char_names = []

    if beat.characters_in_scene and approved_visuals.character_image_map:
        # Use per-character mapping (preferred)
        for char_id in beat.characters_in_scene:
            if char_id in approved_visuals.character_image_map:
                ref = approved_visuals.character_image_map[char_id]
                char_refs.append({
                    "image_base64": ref.image_base64,
                    "mime_type": ref.mime_type,
                })
            # Get character name
            char = next((c for c in story.characters if c.id == char_id), None)
            if char:
                char_names.append(f"{char.name} ({char.age} {char.gender})")
    else:
        # Fallback: use all character images in order
        for i, char in enumerate(story.characters):
            if i < len(approved_visuals.character_images):
                ref = approved_visuals.character_images[i]
                char_refs.append({
                    "image_base64": ref.image_base64,
                    "mime_type": ref.mime_type,
                })
            char_names.append(f"{char.name} ({char.age} {char.gender})")

    # 2. Select location ref for this scene
    location_ref = None
    location_desc = ""
    if beat.location_id and approved_visuals.location_images:
        if beat.location_id in approved_visuals.location_images:
            loc_img = approved_visuals.location_images[beat.location_id]
            location_ref = {
                "image_base64": loc_img.image_base64,
                "mime_type": loc_img.mime_type,
            }
        location_desc = approved_visuals.location_descriptions.get(beat.location_id, "")

    if not location_ref and approved_visuals.setting_image:
        location_ref = {
            "image_base64": approved_visuals.setting_image.image_base64,
            "mime_type": approved_visuals.setting_image.mime_type,
        }
        location_desc = approved_visuals.setting_description or ""

    # 3. Combine all refs (Gemini can handle 5+ reference images)
    all_refs = char_refs[:]
    if location_ref:
        all_refs.append(location_ref)

    print(f"  Scene refs: {len(char_refs)} character(s) + {'1 location' if location_ref else 'no location'} = {len(all_refs)} total input refs")

    # 4. Generate 3 scene refs in parallel with different angle prompts
    if director_script:
        angle_variants = [
            f"{director_script.shot_type}, {director_script.camera_angle}, showing the shot's main composition",
            "character interaction focus, closer framing, intimate perspective",
            "wide establishing shot, slight high angle, environmental context",
        ]
    else:
        angle_variants = [
            "medium shot, eye level, showing full scene composition",
            "over-the-shoulder shot, slightly low angle, intimate perspective",
            "wide establishing shot, slight high angle, environmental context",
        ]

    async def gen_one_ref(angle_desc: str) -> dict:
        # Build scene content from blocks (or legacy fields)
        scene_lines = []
        if beat.blocks:
            for block in beat.blocks:
                if block.type == "description":
                    scene_lines.append(block.text)
                elif block.type == "action":
                    scene_lines.append(block.text)
                elif block.type == "dialogue" and block.character:
                    scene_lines.append(f'{block.character}: "{block.text}"')
        else:
            if beat.description:
                scene_lines.append(beat.description)
            if beat.action:
                scene_lines.append(beat.action)
        scene_content = "\n".join(scene_lines)

        prompt = f"""{style_prefix}

Generate a reference image for this scene.
Scene: {scene_content}
Characters: {', '.join(char_names)}
Location: {location_desc}

Camera: {angle_desc}

Show these exact characters in this exact location.
Maintain character appearances precisely from references.
Maintain location appearance precisely from references.

Portrait orientation, 9:16 aspect ratio."""

        result = await generate_image_with_references(
            prompt=prompt,
            reference_images=all_refs,
            aspect_ratio="9:16",
            resolution="2K",
        )
        return {
            "image_base64": result["image_base64"],
            "mime_type": result.get("mime_type", "image/png"),
        }

    # Run all 3 in parallel
    results = await asyncio.gather(*[gen_one_ref(angle) for angle in angle_variants])
    return list(results)


def _build_char_visual_tags(story: Story) -> dict:
    """Build contrastive visual descriptors so Veo can tell characters apart.

    When characters share the same gender, age becomes the primary differentiator:
      "the young man" vs "the older man"
    When genders differ, gender alone is enough:
      "the woman" vs "the man"
    A distinctive visual detail (hair, clothing) is appended for extra clarity.
    """
    # Count how many characters share each gender
    gender_counts: dict = {}
    for char in story.characters:
        gender_counts[char.gender] = gender_counts.get(char.gender, 0) + 1

    tags = {}
    for char in story.characters:
        # Pick the most distinctive visual detail (hair/clothing/accessory)
        details = [d.strip().lower() for d in char.appearance.split(",") if char.appearance]
        # First detail is often body type — prefer second if available
        visual = details[1] if len(details) > 1 else (details[0] if details else "")

        same_gender_count = gender_counts.get(char.gender, 1)

        if same_gender_count > 1:
            # Multiple characters share this gender — use age to differentiate
            # e.g. "the young man with close-cropped black hair"
            # vs   "the older man with silver-streaked hair"
            if visual:
                tags[char.name] = f"the {char.age} {char.gender} with {visual}"
            else:
                tags[char.name] = f"the {char.age} {char.gender}"
        else:
            # Unique gender — gender alone is distinctive enough
            # e.g. "the woman with red scarf" vs "the man with glasses"
            if visual:
                tags[char.name] = f"the {char.gender} with {visual}"
            else:
                tags[char.name] = f"the {char.gender}"

    return tags


def build_veo_prompt(beat: Beat, story: Story, director_script: Optional[DirectorScript] = None) -> str:
    """Build a concise Veo prompt optimized for reference-image-driven generation.

    With 3 scene refs showing characters + setting, the text prompt shifts from
    DESCRIPTION to DIRECTION: camera, action, dialogue, sound.

    Research-backed approach (Reddit + community guides):
    - 100-150 words max; front-load composition
    - Combine action + dialogue into grounded sentences for attribution
    - Use visual descriptors (clothing/hair), not character names
    - Colon syntax without quotation marks prevents subtitle rendering
    - "focusing on [speaker]" guides camera framing
    - Explicit SOUND section prevents hallucinated audio (studio laughter etc.)
    - End with "No subtitles. No text overlay."
    """
    # Camera direction from director script or defaults
    if director_script:
        shot_type = director_script.shot_type
        camera_movement = director_script.camera_movement
        sound_line = director_script.sound_design
    else:
        ds = default_director_script(beat)
        shot_type = ds.shot_type
        camera_movement = ds.camera_movement
        sound_line = "Ambient sounds matching the scene."

    # Build delivery verb lookup
    delivery_verbs: dict = {}
    if director_script and director_script.dialogue_delivery:
        for dd in director_script.dialogue_delivery:
            if isinstance(dd, dict) and "character" in dd and "verb" in dd:
                delivery_verbs[dd["character"]] = dd["verb"]

    # Build visual descriptor tags for characters
    char_tags = _build_char_visual_tags(story)

    # Extract actions and dialogue from blocks (or legacy fields)
    actions = []
    dialogue_entries = []  # type: list  # (visual_tag, verb, line_text)

    if beat.blocks:
        for block in beat.blocks:
            if block.type == "action":
                actions.append(block.text)
            elif block.type == "dialogue" and block.character:
                verb = delivery_verbs.get(block.character, "says")
                tag = char_tags.get(block.character, block.character)
                dialogue_entries.append((tag, verb, block.text))
    else:
        if beat.action:
            actions.append(beat.action)
        if beat.dialogue:
            for d in beat.dialogue:
                verb = delivery_verbs.get(d.character, "says")
                tag = char_tags.get(d.character, d.character)
                dialogue_entries.append((tag, verb, d.line))

    # Determine primary speaker for camera focus
    primary_speaker_tag = dialogue_entries[0][0] if dialogue_entries else None

    # Build camera line with speaker focus
    if primary_speaker_tag:
        camera_line = f"{shot_type}, {camera_movement}, focusing on {primary_speaker_tag}."
    else:
        camera_line = f"{shot_type}, {camera_movement}."

    parts = [camera_line]

    # Combine first action + first dialogue into a grounded sentence
    # e.g. "The young man steps onto the mat, saying: I didn't come here to bow."
    # This grounds the dialogue to a specific character via shared action.
    if actions and dialogue_entries:
        first_tag, first_verb, first_line = dialogue_entries[0]
        parts.append(f"{first_tag.capitalize()} {actions[0].rstrip('.')}, {first_verb}: {first_line}")
        # Remaining dialogue lines as standalone
        for idx in range(1, len(dialogue_entries)):
            tag, verb, line = dialogue_entries[idx]
            parts.append(f"{tag.capitalize()} {verb}: {line}")
    elif dialogue_entries:
        # No action — dialogue only
        for entry in dialogue_entries:
            tag, verb, line = entry
            parts.append(f"{tag.capitalize()} {verb}: {line}")
    elif actions:
        # No dialogue — action only
        parts.append(actions[0])

    parts.append(f"SOUND: {sound_line}")
    parts.append("No subtitles. No text overlay. Portrait 9:16, 8 seconds.")

    return "\n".join(parts)


async def download_video(video_url: str, film_id: str, shot_number: int) -> str:
    """Download video from Google's authenticated URL and save locally."""
    async with httpx.AsyncClient() as client:
        response = await client.get(
            video_url,
            headers={"x-goog-api-key": GOOGLE_GENAI_API_KEY},
            follow_redirects=True,
            timeout=120.0,
        )
        response.raise_for_status()

        filename = f"{film_id}_shot_{shot_number:02d}.mp4"
        filepath = os.path.join(TEMP_DIR, filename)

        with open(filepath, "wb") as f:
            f.write(response.content)

        return filepath


async def generate_shot_with_retry(
    beat: Beat,
    prompt: str,
    reference_images: List[dict],  # 3 scene-specific refs (required)
    max_retries: int = 2,
) -> dict:
    """Generate a video shot with retry logic.

    Uses a semaphore to respect Veo's 2 RPM rate limit.
    Scene ref generation happens outside this function (no throttle needed).
    """
    sem = _get_veo_semaphore()
    last_error = None

    for attempt in range(max_retries + 1):
        try:
            async with sem:
                print(f"  [Veo] Acquired slot — attempt {attempt + 1}/{max_retries + 1} for shot {beat.number}")
                result = await generate_video(
                    prompt=prompt,
                    reference_images=reference_images,
                    duration_seconds=8,
                    aspect_ratio="9:16",
                )
                # Delay after call to space out RPM usage
                print(f"  [Veo] Shot {beat.number} done, waiting {VEO_DELAY_BETWEEN_CALLS}s for rate limit...")
                await asyncio.sleep(VEO_DELAY_BETWEEN_CALLS)
            return result
        except Exception as e:
            last_error = e
            print(f"  Shot {beat.number} attempt {attempt + 1} failed: {e}")
            if attempt < max_retries:
                # On rate limit errors, wait longer before retry
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    print(f"  Rate limited — waiting 60s before retry...")
                    await asyncio.sleep(60)
                else:
                    await asyncio.sleep(2)

    raise Exception(f"Shot {beat.number} failed after {max_retries + 1} attempts: {last_error}")


# ============================================================
# Background Generation Task
# ============================================================

async def process_single_shot(
    i: int,
    beat: Beat,
    job: "FilmJob",
    director_script: Optional[DirectorScript] = None,
    prompt_override: Optional[str] = None,
    reference_image: Optional[dict] = None,
) -> None:
    """Process a single shot: generate scene refs → generate video → download.

    If reference_image is provided (key moment), skip scene ref generation and use it directly.
    Updates job.completed_shots and cost fields in-place.
    Safe in asyncio (single-threaded, no lock needed).
    """
    desc_preview = (beat.description or "")[:100] if beat.description else "(blocks)"
    print(f"\n--- Shot {i + 1}/{job.total_shots}: Beat {beat.number} ---")
    print(f"Description: {desc_preview}...")
    print(f"Characters: {beat.characters_in_scene}")
    print(f"Location: {beat.location_id}")
    if director_script:
        print(f"Director: {director_script.shot_type}, {director_script.camera_movement}")

    if reference_image:
        # Use provided key moment image as single reference (no scene ref generation)
        scene_refs = [reference_image]
        print(f"[Shot {beat.number}] Using key moment image as reference (no scene ref generation)")
    else:
        # STEP 1: Generate 3 scene-specific reference images via Nano Banana
        print(f"[Shot {beat.number}] Generating 3 scene-specific reference images...")
        scene_refs = await generate_scene_references(
            beat=beat,
            story=job.story,
            approved_visuals=job.approved_visuals,
            director_script=director_script,
        )
        job.cost_scene_refs += COST_IMAGE_GENERATION * SCENE_REFS_PER_SHOT
        print(f"[Shot {beat.number}] Scene refs generated (cost: ${COST_IMAGE_GENERATION * SCENE_REFS_PER_SHOT:.2f})")

    # STEP 2: Generate video shot (use override prompt if provided, else build from scratch)
    shot_prompt = prompt_override if prompt_override else build_veo_prompt(beat, job.story, director_script)
    print(f"[Shot {beat.number}] Generating video with {len(scene_refs)} ref(s)...")
    video_result = await generate_shot_with_retry(
        beat=beat,
        prompt=shot_prompt,
        reference_images=scene_refs,
    )
    job.cost_videos += COST_PER_VIDEO
    print(f"[Shot {beat.number}] Video generated (cost: ${COST_PER_VIDEO:.2f}, total so far: ${job.cost_total:.2f})")

    # STEP 3: Download video
    print(f"[Shot {beat.number}] Downloading video...")
    video_path = await download_video(
        video_result["video_url"],
        job.film_id,
        beat.number,
    )
    print(f"[Shot {beat.number}] Video saved to: {video_path}")

    # Record completed shot
    job.completed_shots.append(CompletedShot(
        number=beat.number,
        video_path=video_path,
        veo_prompt=shot_prompt,
    ))
    job.current_shot = len(job.completed_shots)
    await persist_film_job(job)
    print(f"[Shot {beat.number}] Complete! ({job.current_shot}/{job.total_shots} done)")


async def run_film_generation(
    film_id: str,
    prompt_overrides: Optional[Dict[int, str]] = None,
    shot_references: Optional[Dict[int, dict]] = None,
):
    """Background task to generate all shots in parallel.

    Args:
        film_id: The film job ID
        prompt_overrides: Optional dict of beat_number -> veo_prompt for user-edited prompts
        shot_references: Optional dict of beat_number -> {image_base64, mime_type} key moment refs
    """
    job = film_jobs.get(film_id)
    if not job:
        return

    try:
        print(f"\n{'='*60}")
        print(f"Starting film generation (parallel): {job.film_id}")
        print(f"Total shots: {job.total_shots}")
        print(f"{'='*60}\n")

        # Determine which beats to process
        if prompt_overrides:
            # When user provided edited prompts, only process those beats
            beats_to_process = [b for b in job.story.beats if b.number in prompt_overrides]
            print(f"EDITED PROMPTS: Processing {len(beats_to_process)} beats with user-edited prompts")
            job.total_shots = len(beats_to_process)
        elif MAX_SHOTS_FOR_TESTING is not None:
            beats_to_process = job.story.beats[:MAX_SHOTS_FOR_TESTING]
            print(f"TESTING MODE: Limiting to {MAX_SHOTS_FOR_TESTING} shots")
            job.total_shots = len(beats_to_process)
        else:
            beats_to_process = job.story.beats

        # Generate director scripts (one Gemini call for all scenes)
        print("Generating director scripts...")
        director_scripts = await generate_director_scripts(job.story, beats_to_process)

        # Build a lookup by scene_number
        ds_map: Dict[int, DirectorScript] = {ds.scene_number: ds for ds in director_scripts}

        # Launch all shots in parallel
        job.phase = "filming"
        await persist_film_job(job)
        tasks = [
            process_single_shot(
                i, beat, job, ds_map.get(beat.number),
                prompt_override=prompt_overrides.get(beat.number) if prompt_overrides else None,
                reference_image=shot_references.get(beat.number) if shot_references else None,
            )
            for i, beat in enumerate(beats_to_process)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Check for failures
        failures = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failures.append((beats_to_process[i].number, str(result)))

        if failures:
            failed_shots = ", ".join(f"Shot {num}: {err}" for num, err in failures)
            print(f"WARNING: {len(failures)} shot(s) failed: {failed_shots}")
            if len(failures) == len(beats_to_process):
                raise Exception(f"All shots failed. {failed_shots}")
            print(f"Continuing with {len(job.completed_shots)} successful shots...")

        # Sort completed shots by beat number for proper assembly order
        job.completed_shots.sort(key=lambda s: s.number)

        # Assembly phase
        print(f"\n{'='*60}")
        print("Assembling final video...")
        print(f"{'='*60}\n")

        job.phase = "assembling"
        await persist_film_job(job)
        video_paths = [shot.video_path for shot in job.completed_shots]

        assembly_result = await assemble_videos(video_paths, crossfade_duration=0.0)

        job.final_video_path = assembly_result["output_path"]
        job.status = "ready"
        await persist_film_job(job)

        print(f"\n{'='*60}")
        print(f"Film generation complete!")
        print(f"Final video: {job.final_video_path}")
        print(f"Duration: {assembly_result['duration']}s")
        print(f"{'='*60}\n")

    except Exception as e:
        import traceback
        print(f"\n{'='*60}")
        print(f"Film generation failed!")
        print(f"Error: {e}")
        print(traceback.format_exc())
        print(f"{'='*60}\n")

        job.status = "failed"
        job.error_message = str(e)
        await persist_film_job(job)


# ============================================================
# Preview / Debug Endpoints
# ============================================================

class ShotPromptPreview(BaseModel):
    beat_number: int
    scene_heading: Optional[str] = None
    veo_prompt: str
    characters_in_scene: Optional[List[str]] = None
    location_id: Optional[str] = None


class PreviewPromptsResponse(BaseModel):
    shots: List[ShotPromptPreview]
    estimated_cost_usd: float


class EditedShot(BaseModel):
    beat_number: int
    veo_prompt: str
    reference_image: Optional[ReferenceImage] = None  # Key moment image for this shot


class GenerateWithPromptsRequest(BaseModel):
    story: Story
    approved_visuals: ApprovedVisuals
    key_moment_image: KeyMomentRef
    edited_shots: List[EditedShot]
    generation_id: Optional[str] = None  # Link to generation session


@router.post("/preview-prompts", response_model=PreviewPromptsResponse)
async def preview_prompts(request: GenerateFilmRequest):
    """
    Generate and return Veo prompts for first N beats WITHOUT generating video.
    Calls generate_director_scripts (cheap Gemini Flash call) then build_veo_prompt (pure formatting).
    User can review, edit, and copy these prompts before committing to generation.
    """
    story = request.story

    # If beat_numbers provided (from key moments), use those; else first N
    if request.beat_numbers:
        beat_set = set(request.beat_numbers)
        beats_to_process = [b for b in story.beats if b.number in beat_set]
        # Sort by original order
        beats_to_process.sort(key=lambda b: b.number)
    else:
        beats_to_process = story.beats[:MAX_SHOTS_FOR_TESTING] if MAX_SHOTS_FOR_TESTING else story.beats

    # Generate director scripts (one Gemini Flash call, ~$0.001)
    director_scripts = await generate_director_scripts(story, beats_to_process)
    ds_map = {ds.scene_number: ds for ds in director_scripts}

    shots = []
    for beat in beats_to_process:
        ds = ds_map.get(beat.number)
        veo_prompt = build_veo_prompt(beat, story, ds)
        shots.append(ShotPromptPreview(
            beat_number=beat.number,
            scene_heading=beat.scene_heading,
            veo_prompt=veo_prompt,
            characters_in_scene=beat.characters_in_scene,
            location_id=beat.location_id,
        ))

    # Cost estimate: video generation only (key moments already serve as refs)
    num_shots = len(shots)
    estimated_cost = num_shots * COST_PER_VIDEO

    return PreviewPromptsResponse(
        shots=shots,
        estimated_cost_usd=round(estimated_cost, 2),
    )


@router.post("/generate-with-prompts", response_model=GenerateFilmResponse)
async def generate_film_with_prompts(request: GenerateWithPromptsRequest, background_tasks: BackgroundTasks):
    """
    Start film generation using user-edited Veo prompts.
    Same as /generate but uses provided prompts instead of build_veo_prompt().
    """
    film_id = uuid.uuid4().hex[:12]

    # Filter beats to only the ones we have prompts for
    prompt_map = {s.beat_number: s.veo_prompt for s in request.edited_shots}
    beats_to_process = [b for b in request.story.beats if b.number in prompt_map]

    job = FilmJob(
        film_id=film_id,
        status="generating",
        created_at=datetime.now(),
        story=request.story,
        approved_visuals=request.approved_visuals,
        key_moment_image=request.key_moment_image,
        total_shots=len(beats_to_process),
        generation_id=request.generation_id,
    )

    film_jobs[film_id] = job
    await persist_film_job(job)

    # Build per-shot reference images from key moments (if provided)
    shot_refs: Optional[Dict[int, dict]] = None
    ref_map = {s.beat_number: s.reference_image for s in request.edited_shots if s.reference_image}
    if ref_map:
        shot_refs = {
            bn: {"image_base64": ref.image_base64, "mime_type": ref.mime_type}
            for bn, ref in ref_map.items()
        }

    # Start generation in background with prompt overrides + per-shot refs
    background_tasks.add_task(run_film_generation, film_id, prompt_map, shot_refs)

    return GenerateFilmResponse(
        film_id=film_id,
        status="generating",
        total_shots=job.total_shots,
    )


# ============================================================
# Endpoints
# ============================================================

@router.post("/generate", response_model=GenerateFilmResponse)
async def generate_film(request: GenerateFilmRequest, background_tasks: BackgroundTasks):
    """
    Start film generation from approved story and visuals.

    Returns immediately with a film_id to poll for status.
    """
    film_id = uuid.uuid4().hex[:12]

    job = FilmJob(
        film_id=film_id,
        status="generating",
        created_at=datetime.now(),
        story=request.story,
        approved_visuals=request.approved_visuals,
        key_moment_image=request.key_moment_image,
        total_shots=len(request.story.beats),
        generation_id=request.generation_id,
    )

    film_jobs[film_id] = job
    await persist_film_job(job)

    # Start generation in background
    background_tasks.add_task(run_film_generation, film_id)

    return GenerateFilmResponse(
        film_id=film_id,
        status="generating",
        total_shots=job.total_shots,
    )


@router.get("/{film_id}", response_model=FilmStatusResponse)
async def get_film_status(film_id: str):
    """
    Poll film generation status. Checks memory first, falls back to DB.
    """
    job = film_jobs.get(film_id)

    if job:
        # Serve from in-memory (active generation)
        completed_shots = [
            CompletedShotInfo(
                number=shot.number,
                preview_url=f"/film/{film_id}/shot/{shot.number}",
                veo_prompt=shot.veo_prompt,
            )
            for shot in job.completed_shots
        ]
        return FilmStatusResponse(
            film_id=job.film_id,
            status=job.status,
            current_shot=job.current_shot,
            total_shots=job.total_shots,
            phase=job.phase,
            completed_shots=completed_shots,
            final_video_url=f"/film/{film_id}/final" if job.final_video_path else None,
            error_message=job.error_message,
            cost=CostBreakdown(
                scene_refs_usd=round(job.cost_scene_refs, 4),
                videos_usd=round(job.cost_videos, 4),
                total_usd=round(job.cost_total, 4),
            ),
        )

    # No DB fallback — persistence is on frontend (Supabase)
    raise HTTPException(status_code=404, detail="Film not found")


@router.get("/{film_id}/shot/{shot_number}")
async def get_shot_preview(film_id: str, shot_number: int):
    """
    Stream a completed shot video. Checks memory first, falls back to DB.
    """
    video_path = None

    job = film_jobs.get(film_id)
    if job:
        shot = next((s for s in job.completed_shots if s.number == shot_number), None)
        if shot:
            video_path = shot.video_path

    if not video_path:
        raise HTTPException(status_code=404, detail="Shot not found")
    if not os.path.exists(video_path):
        raise HTTPException(status_code=404, detail="Video file not found")

    return FileResponse(
        video_path,
        media_type="video/mp4",
        filename=f"shot_{shot_number:02d}.mp4",
    )


@router.get("/{film_id}/final")
async def get_final_video(film_id: str):
    """
    Stream the final assembled video. Checks memory first, falls back to DB.
    """
    final_path = None
    title = "film"

    job = film_jobs.get(film_id)
    if not job:
        raise HTTPException(status_code=404, detail="Film not found")
    if job.status != "ready" or not job.final_video_path:
        raise HTTPException(status_code=400, detail="Film not ready yet")
    final_path = job.final_video_path
    title = job.story.title

    if not final_path or not os.path.exists(final_path):
        raise HTTPException(status_code=404, detail="Video file not found")

    safe_title = "".join(c for c in title if c.isalnum() or c in " -_").strip()
    filename = f"{safe_title or 'film'}.mp4"

    return FileResponse(
        final_path,
        media_type="video/mp4",
        filename=filename,
    )


@router.post("/{film_id}/shot/{shot_number}/regenerate", response_model=RegenerateShotResponse)
async def regenerate_shot(
    film_id: str,
    shot_number: int,
    request: RegenerateShotRequest,
    background_tasks: BackgroundTasks
):
    """
    Regenerate a specific shot with optional feedback.

    The shot will be regenerated using the same beat but with modified prompt if feedback provided.
    """
    job = film_jobs.get(film_id)
    if not job:
        raise HTTPException(status_code=404, detail="Film not found")

    if job.status not in ["ready", "failed"]:
        raise HTTPException(status_code=400, detail="Film must be ready or failed to regenerate shots")

    # Find the beat for this shot number
    beat = next((b for b in job.story.beats if b.number == shot_number), None)
    if not beat:
        raise HTTPException(status_code=404, detail=f"Beat {shot_number} not found")

    # Start regeneration in background
    background_tasks.add_task(
        run_shot_regeneration,
        job,
        beat,
        request.feedback
    )

    return RegenerateShotResponse(
        film_id=film_id,
        shot_number=shot_number,
        status="regenerating",
        preview_url=f"/film/{film_id}/shot/{shot_number}",
    )


async def run_shot_regeneration(job: FilmJob, beat: Beat, feedback: Optional[str]):
    """Background task to regenerate a single shot using per-scene reference generation."""
    try:
        print(f"\n{'='*60}")
        print(f"Regenerating shot {beat.number} for film {job.film_id}")
        if feedback:
            print(f"Feedback: {feedback}")
        print(f"{'='*60}\n")

        # Generate director script for this beat
        print("Generating director script...")
        ds_list = await generate_director_scripts(job.story, [beat])
        ds = ds_list[0] if ds_list else default_director_script(beat)

        # Generate 3 scene-specific reference images
        print("Generating 3 scene-specific reference images...")
        scene_refs = await generate_scene_references(
            beat=beat,
            story=job.story,
            approved_visuals=job.approved_visuals,
            director_script=ds,
        )
        job.cost_scene_refs += COST_IMAGE_GENERATION * SCENE_REFS_PER_SHOT

        # Generate video shot with director script + negative prompt
        shot_prompt = build_veo_prompt(beat, job.story, ds)
        if feedback:
            shot_prompt += f"\n\nADJUSTMENT: {feedback}"

        print(f"Generating video shot with {len(scene_refs)} scene refs...")
        video_result = await generate_shot_with_retry(
            beat=beat,
            prompt=shot_prompt,
            reference_images=scene_refs,
        )
        job.cost_videos += COST_PER_VIDEO

        # Download video
        video_path = await download_video(
            video_result["video_url"],
            job.film_id,
            beat.number,
        )

        # Update completed shots
        existing_shot = next((s for s in job.completed_shots if s.number == beat.number), None)
        if existing_shot:
            # Delete old video file
            if os.path.exists(existing_shot.video_path):
                os.remove(existing_shot.video_path)
            existing_shot.video_path = video_path
            existing_shot.veo_prompt = shot_prompt
        else:
            job.completed_shots.append(CompletedShot(
                number=beat.number,
                video_path=video_path,
                veo_prompt=shot_prompt,
            ))

        # Re-assemble the film
        print("Re-assembling film with new shot...")
        job.completed_shots.sort(key=lambda s: s.number)
        video_paths = [shot.video_path for shot in job.completed_shots]

        assembly_result = await assemble_videos(video_paths, crossfade_duration=0.0)
        job.final_video_path = assembly_result["output_path"]
        job.status = "ready"
        await persist_film_job(job)

        print(f"Shot {beat.number} regenerated successfully!")

    except Exception as e:
        import traceback
        print(f"Shot regeneration failed: {e}")
        print(traceback.format_exc())
        job.error_message = f"Shot {beat.number} regeneration failed: {e}"
        await persist_film_job(job)
